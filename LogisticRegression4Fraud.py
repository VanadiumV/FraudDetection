# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j6X6eyuEWJZyuF85IXA-boSn76uVwrs_
"""

!pip install seaborn
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
plt.style.use("seaborn")

#Step1 generate toy(Dummy)Dataset

X,y =make_blobs(n_samples=2000,n_features=2,cluster_std=3,centers=2,random_state=42)
n_features=2 # helper fxns down
print(X.shape,y.shape)

print(y)

#Step-2 Visualize Dataset
def visualise(X,y):
  plt.scatter(X[:,0],X[:,1],c=y,cmap="viridis")
  plt.show()

visualise(X,y)

#Step-3
def normalise(X):
  u=X.mean(axis=0)
  std=X.std(axis=0)

  return (X-u)/std

X = normalise(X)

visualise(X,y)

#Step 4 Train Test Split

XT,Xt,yT,yt = train_test_split(X,y,test_size=0.25,shuffle=False,random_state=0)
print(XT.shape,yT.shape)
print(Xt.shape,yt.shape)

visualise(XT,yT)
visualise(Xt,yt)

"""Hypothesis fxn in modelling -> h0(x)=sigma(theta raise d to the power transpose *x)=1/1+e^-0T*x"""

#model
def sigmoid(x):
  return 1/(1 + np.exp(-x))

#HELPER FXNS

def hypothesis(X,theta):
  return sigmoid(np.dot(X,theta))

  #Binary Cross Entropy
def error(y,yp):
    loss = -np.mean(y*np.log(yp) + (1-y)*np.log(1-yp))
    return loss

#Gradient
def gradient(X,y,yp):
  m = X.shape[0]
  grad= -(1/m)*np.dot(X.T, (y-yp))
  return grad

def train(X,y,max_iters=100,learning_rate=0.1):
#RANDOMLY INITIALIZED THETA
    theta = np.random.randn(n_features + 1,1)

    error_list=[]

    for i in range(max_iters):
       yp = hypothesis(X,theta)
       e=error(y,yp)
       grad=gradient(X,y,yp)
       theta = theta - learning_rate*grad

    plt.plot(error_list)
    return theta

def addExtraColumn(X):
  if X.shape[1] == n_features:
    ones=np.ones((X.shape[0],1))
    X = np.hstack((ones,X))

  return X

Xt= addExtraColumn(Xt)
print(Xt)

XT= addExtraColumn(XT)
print(XT)

"""DN S.T * is compatible so thatt we don't need to handle theta not **separately**"""

print(XT.shape)

yT=yT.reshape(-1,1)
yt=yt.reshape(-1,1)

print(yT.shape)
print(yt.shape)

theta = train(XT,yT)

theta = train(XT,yT,max_iters=300,learning_rate=0.2)

theta